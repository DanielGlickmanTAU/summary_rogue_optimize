training summarizaition model: run_summarization.py output is trained model at --output_dir
--output_dir ./models/xsum/100/facebook/bart-base/1e-05
--model_name_or_path facebook/bart-base --dataset_name xsum
--overwrite_output_dir False
--num_train_epochs 3 --evaluation_strategy epoch --max_predict_samples 1000 --per_device_train_batch_size 4 --per_device_eval_batch_size 8  --do_train True --do_eval True --do_predict True --max_eval_samples 128 --num_beams 4  --gradient_accumulation_steps 2 --load_best_model_at_end True --predict_with_generate True --save_total_limit 1 --greater_is_better True --metric_for_best_model rouge2 --max_train_samples 100 --learning_rate 1e-05



running algorithm with loading generated dataset on unsupervised data(to save time)
algorithm.py --amount_to_pass_filter 0.05 --load_generated_model True --ranking oracle --num_train_epochs 5 --evaluation_strategy epoch --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --overwrite_output_dir True --do_train True --do_eval True --do_predict True --max_eval_samples 20 --max_train_samples 32 --max_predict_samples 1000 --max_unsupervised_samples 0 --num_beams 2 --model_name_or_path facebook/bart-base --dataset_name xsum --gradient_accumulation_steps 2 --load_best_model_at_end True --predict_with_generate True --save_total_limit 1 --greater_is_better False --metric_for_best_model loss --learning_rate 1e-05 --output_dir ./out_dirddddasd/ --track_experiment False