In Progess:
replace list[num_beams][input_ids_tensor] into tensor of shape(num_beams,input_ids_tensor) // use torch.stack.. in collator i think

restore more than one gpu

positional_encoding ???

handle all required data to pass to model..
be able to call validation(returns dict).. and add validation set to training ranker
can probably remove my padding if using DataCollatorForTokenClassification


run 10k validation beam search




class roberaranker(roberaForrSEQClassification): with predict select top method

implement tokenize.decode for list when I need it..

make ranker work with <text><summary>:
    3) map get_generated_summaries_with_rouge result:
         ds.map(lambda examples:{'bla':sum([artile + tokenizer.sep + hl for hl in generated_hls]  for (article,generated_hls) in zip(exmaples['artile'],examples['generated_highlights']),[])
         .map(normalize scores)
         .map(tokenize...)
    4) carefully debug the input into the network
    5) overfit 100 examples on regression





ranker : normalize outputs? per sample?(regression)

try:
    try regular fine tuning..overfit


DONE:
test that not all attention_mask is 1, some should be zero for padding




fix issues with long articles...
I can pospone full length for now..
^ change no_repeat_ngram_size to 3?

tried:
    select always above average

    append to input text "summarize: " | DOESNT WORK

    force_bos_token_to_be_generated=False | DOESNT MATTER
