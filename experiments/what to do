fix length: roberta max length is 512

in prediciton_loop, loss is empty



different loss
In Progess:
replace list[num_beams][input_ids_tensor] into tensor of shape(num_beams,input_ids_tensor) // use torch.stack.. in collator i think


restore more than one gpu




class roberaranker(roberaForrSEQClassification): with predict select top method

implement tokenize.decode for list when I need it..

make ranker work with <text><summary>:
    3) map get_generated_summaries_with_rouge result:
         ds.map(lambda examples:{'bla':sum([artile + tokenizer.sep + hl for hl in generated_hls]  for (article,generated_hls) in zip(exmaples['artile'],examples['generated_highlights']),[])
         .map(normalize scores)
         .map(tokenize...)
    4) carefully debug the input into the network
    5) overfit 100 examples on regression





ranker : normalize outputs? per sample?(regression)

try:
    try regular fine tuning..overfit


DONE:
test that not all attention_mask is 1, some should be zero for padding




fix issues with long articles...
I can pospone full length for now..
^ change no_repeat_ngram_size to 3?

tried:
    select always above average

    append to input text "summarize: " | DOESNT WORK

    force_bos_token_to_be_generated=False | DOESNT MATTER
