In Progess:
overwrite roberta and see what I get in forward

it may be possible to just override forward of RObertaForClassification and return my loss.
 careful with from_pretrained ========= try MyCustomRoberta(RObertaConfig.from_pretrained(..))

 another option is to implement CustomLoss(nn.module); composition over inheritance


 if this doesnt work,, overwrite huggingface trainer:compute_loss

class roberaranker(roberaForrSEQClassification): with predict select top method

implement tokenize.decode for list when I need it..

make ranker work with <text><summary>:
    3) map get_generated_summaries_with_rouge result:
         ds.map(lambda examples:{'bla':sum([artile + tokenizer.sep + hl for hl in generated_hls]  for (article,generated_hls) in zip(exmaples['artile'],examples['generated_highlights']),[])
         .map(normalize scores)
         .map(tokenize...)
    4) carefully debug the input into the network
    5) overfit 100 examples on regression





ranker : normalize outputs? per sample?(regression)

try:
    try regular fine tuning..overfit


DONE:
test that not all attention_mask is 1, some should be zero for padding




fix issues with long articles...
I can pospone full length for now..
^ change no_repeat_ngram_size to 3?

tried:
    select always above average

    append to input text "summarize: " | DOESNT WORK

    force_bos_token_to_be_generated=False | DOESNT MATTER
