turning off classifier(last layer) dropout

ensure that also base model(roberta) weights are changing
overfit on a single example with 6 beams..
try log sigmoid loss of 2 samples?


algorithm limitations:
max length is 512
validation=train
n_beams = 2

implementation limitations:
more than 1 batch
more than 1 gpu



different loss
In Progess:
replace list[num_beams][input_ids_tensor] into tensor of shape(num_beams,input_ids_tensor) // use torch.stack.. in collator i think

if labels are equal, skip loss


class roberaranker(roberaForrSEQClassification): with predict select top method


    4) carefully debug the input into the network
    5) overfit 100 examples on regression





ranker : normalize outputs? per sample?(regression)

try:
    try regular fine tuning..overfit


DONE:
test that not all attention_mask is 1, some should be zero for padding




fix issues with long articles...
I can pospone full length for now..
^ change no_repeat_ngram_size to 3?

tried:
    select always above average

    append to input text "summarize: " | DOESNT WORK

    force_bos_token_to_be_generated=False | DOESNT MATTER
